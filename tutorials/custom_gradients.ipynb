{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifying Custom Gradients\n",
    "For instructions on how to run these tutorial notebooks, please see the [index](./index.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving a Simple Mathematical Program\n",
    "In this case, we are optimizing a simple 2-variable cost function `x_0 + 2 * x_1`, where both variables are constrained to be in the range `[0..1]`.\n",
    "\n",
    "Without any additional constraints, we can find the optimal solution to be `x_0 = x_1 = 0`, which yields a cost of `0`.\n",
    "\n",
    "By adding a constraint that the solution cannot lie on a circle centered at the origin with radius 0.5, we instead get an optimal solution of `x_0 = 0.5` and `x_1 = 0.0`, which yields a cost of `0.5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success?  True\n",
      "SolutionResult.kSolutionFound\n",
      "x*=  [0.50000004 0.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pydrake.solvers import MathematicalProgram, Solve\n",
    "\n",
    "prog = MathematicalProgram()\n",
    "x = prog.NewContinuousVariables(2)\n",
    "prog.AddCost(x[0] + 2.0 * x[1])\n",
    "prog.AddBoundingBoxConstraint(0.0, 1.0, x)\n",
    "prog.AddConstraint(x[0]**2 + x[1]**2 >= 0.5**2)\n",
    "\n",
    "result = Solve(prog, np.array([1.0, 1.0]))\n",
    "print(\"Success? \", result.is_success())\n",
    "print(result.get_solution_result())\n",
    "print(\"x*= \", result.GetSolution(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Providing Your Own Gradients\n",
    "For simple expressions like these, AutoDiff already took care of calculating all the gradients for optimization.\n",
    "\n",
    "However, assume you have a more complex function that cannot accept these `AutoDiffXd` object that get created by Drake. In this case, you can manually specify gradients yourself, for both cost and constraint expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success?  True\n",
      "SolutionResult.kSolutionFound\n",
      "x*=  [0.50000007 0.        ]\n"
     ]
    }
   ],
   "source": [
    "from pydrake.autodiffutils import AutoDiffXd, ExtractValue\n",
    "\n",
    "def cost_expression(x):\n",
    "    if x.dtype == float:\n",
    "        # The cost expression with floats.\n",
    "        return x[0] + 2.0 * x[1]\n",
    "    else:\n",
    "        # Extract the values from the AutoDiffXd object\n",
    "        x_value = ExtractValue(x)\n",
    "\n",
    "        # Compute the cost and gradient\n",
    "        # NOTE: x_value is a column vector so we specify two indices to avoid warnings\n",
    "        cost = x_value[0,0] + 2.0 * x_value[1,0]\n",
    "        gradient = np.array([1.0, 2.0])\n",
    "\n",
    "        # Return the new AutoDiffXd object with the cost and gradient\n",
    "        return AutoDiffXd(cost, gradient)\n",
    "\n",
    "def constraint_expression(x):\n",
    "    if x.dtype == float:\n",
    "        # The constraint expression with floats\n",
    "        return x[0]**2 + x[1]**2\n",
    "    else:\n",
    "        # Extract the values from the AutoDiffXd object\n",
    "        x_value = ExtractValue(x)\n",
    "\n",
    "        # Compute the constraint and gradient\n",
    "        # NOTE: x_value is a column vector so we specify two indices to avoid warnings\n",
    "        constraint = x_value[0, 0]**2 + x_value[1, 0]**2\n",
    "        gradient = np.array([2.0 * x_value[0, 0], 2.0 * x_value[1, 0]])\n",
    "\n",
    "        # Return the new AutoDiffXd object with the constraint and gradient\n",
    "        # NOTE: In this case, we have to wrap the expression in a numpy array\n",
    "        return np.array([AutoDiffXd(constraint, gradient)])\n",
    "\n",
    "prog = MathematicalProgram()\n",
    "x = prog.NewContinuousVariables(2)\n",
    "prog.AddCost(cost_expression, x)\n",
    "prog.AddBoundingBoxConstraint(0.0, 1.0, x)\n",
    "\n",
    "# For an expression-based constraint, the lower and upper bounds must also be numpy arrays\n",
    "lb = 0.5**2 * np.ones(1)\n",
    "ub = np.inf * np.ones(1)\n",
    "prog.AddConstraint(constraint_expression, lb, ub, x)\n",
    "\n",
    "result = Solve(prog, np.array([1.0, 1.0]))\n",
    "print(\"Success? \", result.is_success())\n",
    "print(result.get_solution_result())\n",
    "print(\"x*= \", result.GetSolution(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Gradients using Finite Differencing\n",
    "In other cases, you may not have analytical gradients to provide to the optimizer.\n",
    "\n",
    "You can always rely on *finite differencing*, or numerical methods, to approximate the gradients by evaluating expressions at places slightly perturbed from the nominal value.\n",
    "\n",
    "Please note that you should only rely on finite differencing when absolutely necessary, as this can drastically slow down the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success?  True\n",
      "SolutionResult.kSolutionFound\n",
      "x*=  [0.50000007 0.        ]\n"
     ]
    }
   ],
   "source": [
    "from pydrake.math import ComputeNumericalGradient, NumericalGradientMethod, NumericalGradientOption\n",
    "\n",
    "def cost_fn(x):\n",
    "    # This must return a numpy array to be compatible with numerical gradient extraction\n",
    "    return np.array([x[0] + 2.0 * x[1]])\n",
    "\n",
    "def cost_expression(x):\n",
    "    if x.dtype == float:\n",
    "        # The cost expression with floats.\n",
    "        return x[0] + 2.0 * x[1]\n",
    "    else:\n",
    "        # Extract the values from the AutoDiffXd object\n",
    "        x_value = ExtractValue(x)\n",
    "\n",
    "        # Compute the cost and approximate gradient\n",
    "        # We are using forward differencing here as the `option` argument.\n",
    "        cost = cost_fn(x_value)\n",
    "        gradient = ComputeNumericalGradient(cost_fn, x_value, option=NumericalGradientOption(NumericalGradientMethod.kForward))\n",
    "\n",
    "        # Return the new AutoDiffXd object with the cost and gradient\n",
    "        return AutoDiffXd(cost[0,0], gradient[0,:])\n",
    "\n",
    "def constraint_fn(x):\n",
    "    # This must return a numpy array to be compatible with numerical gradient extraction\n",
    "    return np.array([x[0]**2 + x[1]**2])\n",
    "\n",
    "def constraint_expression(x):\n",
    "    if x.dtype == float:\n",
    "        # The constraint expression with floats\n",
    "        return x[0]**2 + x[1]**2\n",
    "    else:\n",
    "        # Extract the values from the AutoDiffXd object\n",
    "        x_value = ExtractValue(x)\n",
    "\n",
    "        # Compute the constraint and approximate gradient\n",
    "        # We are using central differencing here as the `option` argument.\n",
    "        constraint = constraint_fn(x_value)\n",
    "        gradient = ComputeNumericalGradient(constraint_fn, x_value, option=NumericalGradientOption(NumericalGradientMethod.kCentral))\n",
    "        \n",
    "        # Return the new AutoDiffXd object with the constraint and gradient\n",
    "        return np.array([AutoDiffXd(constraint[0,0], gradient[0,:])])\n",
    "                                  \n",
    "prog = MathematicalProgram()\n",
    "x = prog.NewContinuousVariables(2)\n",
    "prog.AddCost(cost_expression, x)\n",
    "prog.AddBoundingBoxConstraint(0.0, 1.0, x)\n",
    "\n",
    "# For an expression-based constraint, the lower and upper bounds must also be numpy arrays\n",
    "lb = 0.5**2 * np.ones(1)\n",
    "ub = np.inf * np.ones(1)\n",
    "prog.AddConstraint(constraint_expression, lb, ub, x)\n",
    "\n",
    "result = Solve(prog, np.array([1.0, 1.0]))\n",
    "print(\"Success? \", result.is_success())\n",
    "print(result.get_solution_result())\n",
    "print(\"x*= \", result.GetSolution(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
